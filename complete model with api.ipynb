{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sagar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sagar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sagar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sagar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sagar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sagar/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array, asarray, zeros\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import pickle\n",
    "import keras\n",
    "from keras import backend\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "\n",
    "#Ignoring the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Embedding, Input, Dense, LSTM, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.layers import LeakyReLU\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_contractions(sentence):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(sentence)\n",
    "\n",
    "  \n",
    "def words_list(sample):\n",
    "    words = []\n",
    "    \"\"\"Tokenising the corpus\"\"\"\n",
    "    for i in sample: \n",
    "        temp = []\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        temp = normalize(temp)\n",
    "        words.append(temp)\n",
    "    return words\n",
    "\n",
    "  \n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    \"\"\"This is the main function which takes all other functions to pre-process the data given\"\"\"\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    lemmatize_words(words)\n",
    "    return words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize the words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dictionary of Class names and their index as in labels\n",
    "label_name = {0: 'Acupuncture', \n",
    "              1: 'Chiropractic',\n",
    "              2: 'Diagnostic Lab',\n",
    "              3: 'Emergency Services',\n",
    "              4: 'Human Resource Contact',\n",
    "              5: 'Maternity',\n",
    "              6: 'MRI-CAT Scan',\n",
    "              7: 'Pharmacy Benefit Manager',\n",
    "              8: 'Physician Visit Office Sick',\n",
    "              9: 'Psychiatric Out-patient'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,jsonify,request\n",
    "app = Flask(__name__)\n",
    "textcnn = load_model(\"./models/Text_CNN.h5\")\n",
    "with open(\"./pickles/Text_CNN.pickle\", 'rb') as handle:\n",
    "    textcnn_pickle = pickle.load(handle)\n",
    "\n",
    "# textrnn = load_model(\"./models/Text_RNN.h5\")\n",
    "# with open(\"./pickles/Text_RNN.pickle\", 'rb') as handle:\n",
    "#     textrnn_pickle = pickle.load(handle)\n",
    "@app.route(\"/\", methods=[\"POST\"])\n",
    "def index():\n",
    "    req_data = request.get_json()\n",
    "    s = req_data['sentence']\n",
    "    word_list = words_list(s)\n",
    "    X_test = []\n",
    "    for list_of_words in word_list:\n",
    "        sentence = ' '.join(x for x in list_of_words)\n",
    "        X_test.append(sentence)\n",
    "    encoded_word_list1 = textcnn_pickle.texts_to_sequences(X_test)\n",
    "    encoded_word_list2 = textrnn_pickle.texts_to_sequences(X_test)\n",
    "#     Xrnn = pad_sequences(encoded_word_list, maxlen=20, padding='pre')\n",
    "    Xcnn = pad_sequences(encoded_word_list, maxlen=15, padding='pre')\n",
    "#     predrnn=textrnn.predict(Xrnn)\n",
    "#     predrnn=np.argmax(predrnn)\n",
    "    predcnn=textcnn.predict(Xcnn)\n",
    "    predcnn=np.argmax(predcnn)\n",
    "    return jsonify({\n",
    "                   \"predcnn\":str(label_name[predcnn])\n",
    "                   })\n",
    "if __name__ == '__main__':\n",
    "\tapp.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0  65 379]]\n",
      "Maternity\n"
     ]
    }
   ],
   "source": [
    "textcnn = load_model(\"./models/Text_CNN.h5\")\n",
    "with open(\"./pickles/Text_CNN.pickle\", 'rb') as handle:\n",
    "    textcnn_pickle = pickle.load(handle)\n",
    "s1=['i am pregnant complications']\n",
    "word_list = words_list(s1)\n",
    "X_test = []\n",
    "for list_of_words in word_list:\n",
    "    sentence = ' '.join(x for x in list_of_words)\n",
    "    X_test.append(sentence)\n",
    "encoded_word_list = textcnn_pickle.texts_to_sequences(X_test)\n",
    "X = pad_sequences(encoded_word_list, maxlen=15, padding='pre')\n",
    "print(X)\n",
    "textcnn._make_predict_function()\n",
    "pred = textcnn.predict(X)\n",
    "pred = np.argmax(pred)\n",
    "print(label_name[pred])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
